{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46619dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Label</th>\n",
       "      <th>Partitioned Abstract</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enhancing Investment Analysis: Optimizing AI-A...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Han, XW; Wang, N; Che, SK; Yang, HY; Zhang, KP...</td>\n",
       "      <td>AI and Finance</td>\n",
       "      <td>recent years application generative artificial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial intelligence applications in supply...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Pournader, M; Ghaderi, H; Hassanzadegan, A; Fa...</td>\n",
       "      <td>AI and Supply Chain Management</td>\n",
       "      <td>paper presents systematic review studies relat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forecasting disruptions in global food value c...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tamasiga, P; Ouassou, E; Onyeaka, H; Bakwena, ...</td>\n",
       "      <td>AI and Supply Chain Management</td>\n",
       "      <td>globalization interconnected supply chains led...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI-powered marketing: What, where, and how?*</td>\n",
       "      <td>2024</td>\n",
       "      <td>Kumar, V; Ashraf, AR; Nadeem, W</td>\n",
       "      <td>AI and Marketing</td>\n",
       "      <td>artificial intelligence ai become disruptive f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence based decision-making ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Lehner, OM; Ittonen, K; Silvola, H; Strm, E; ...</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>trustworthiness using rest's component model a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>The Big Data, Artificial Intelligence, and Blo...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Gusc, J; Bosma, P; Jarka, S; Biernat-Jarka, A</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>current energy prices include environmental so...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>The Artificial Intelligence Revolution in Digi...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Al-Baity, HH</td>\n",
       "      <td>AI and Finance</td>\n",
       "      <td>artificial intelligence ai proliferated last y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>ROLE OF GENETIC-VARIATION AT THE APO AI-CIII-A...</td>\n",
       "      <td>1993</td>\n",
       "      <td>XU, CF; ANGELICO, F; DELBEN, M; HUMPHRIES, S</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>cholesterol diet however significant associati...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Exploring volatility interconnections between ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yousaf, I; Ijaz, MS; Umar, M; Li, YS</td>\n",
       "      <td>AI and Economics</td>\n",
       "      <td>energy artificial intelligence ai two top fiel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>Boosting the efficacy of green accounting for ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Khan, S; Gupta, S</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>accounting practices positively impact current...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Year  \\\n",
       "0    Enhancing Investment Analysis: Optimizing AI-A...  2024   \n",
       "1    Artificial intelligence applications in supply...  2021   \n",
       "2    Forecasting disruptions in global food value c...  2023   \n",
       "3         AI-powered marketing: What, where, and how?*  2024   \n",
       "4    Artificial intelligence based decision-making ...  2022   \n",
       "..                                                 ...   ...   \n",
       "738  The Big Data, Artificial Intelligence, and Blo...  2022   \n",
       "739  The Artificial Intelligence Revolution in Digi...  2023   \n",
       "740  ROLE OF GENETIC-VARIATION AT THE APO AI-CIII-A...  1993   \n",
       "741  Exploring volatility interconnections between ...  2024   \n",
       "742  Boosting the efficacy of green accounting for ...  2025   \n",
       "\n",
       "                                               Authors  \\\n",
       "0    Han, XW; Wang, N; Che, SK; Yang, HY; Zhang, KP...   \n",
       "1    Pournader, M; Ghaderi, H; Hassanzadegan, A; Fa...   \n",
       "2    Tamasiga, P; Ouassou, E; Onyeaka, H; Bakwena, ...   \n",
       "3                      Kumar, V; Ashraf, AR; Nadeem, W   \n",
       "4    Lehner, OM; Ittonen, K; Silvola, H; Strm, E; ...   \n",
       "..                                                 ...   \n",
       "738      Gusc, J; Bosma, P; Jarka, S; Biernat-Jarka, A   \n",
       "739                                       Al-Baity, HH   \n",
       "740       XU, CF; ANGELICO, F; DELBEN, M; HUMPHRIES, S   \n",
       "741               Yousaf, I; Ijaz, MS; Umar, M; Li, YS   \n",
       "742                                  Khan, S; Gupta, S   \n",
       "\n",
       "                              Label  \\\n",
       "0                    AI and Finance   \n",
       "1    AI and Supply Chain Management   \n",
       "2    AI and Supply Chain Management   \n",
       "3                  AI and Marketing   \n",
       "4                 AI and Accounting   \n",
       "..                              ...   \n",
       "738               AI and Accounting   \n",
       "739                  AI and Finance   \n",
       "740               AI and Accounting   \n",
       "741                AI and Economics   \n",
       "742               AI and Accounting   \n",
       "\n",
       "                                  Partitioned Abstract  target  \n",
       "0    recent years application generative artificial...       1  \n",
       "1    paper presents systematic review studies relat...       0  \n",
       "2    globalization interconnected supply chains led...       0  \n",
       "3    artificial intelligence ai become disruptive f...       2  \n",
       "4    trustworthiness using rest's component model a...       4  \n",
       "..                                                 ...     ...  \n",
       "738  current energy prices include environmental so...       4  \n",
       "739  artificial intelligence ai proliferated last y...       1  \n",
       "740  cholesterol diet however significant associati...       4  \n",
       "741  energy artificial intelligence ai two top fiel...       3  \n",
       "742  accounting practices positively impact current...       4  \n",
       "\n",
       "[743 rows x 6 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words transformation of the partitioned abstracts\n",
    "# This script uses the CountVectorizer from sklearn to transform the partitioned abstracts into a bag of words representation.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Gets the data from Data/partitioned_abstracts.csv and puts it into a pandas dataframe\n",
    "df = pd.read_csv('Data/Partitioned_Abstracts.csv', sep=',', header=0, encoding='utf-8')\n",
    "\n",
    "# Add target column to the dataframe based on label\n",
    "label_mapping = {\n",
    "    'AI and Supply Chain Management': 0,\n",
    "    'AI and Finance': 1,\n",
    "    'AI and Marketing': 2,\n",
    "    'AI and Economics': 3,\n",
    "    'AI and Accounting': 4\n",
    "}\n",
    "df['target'] = df['Label'].map(label_mapping)\n",
    "\n",
    "#randomize the order of the dataframe using random.shuffle\n",
    "random.seed(42)  # For reproducibility\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#remove partitions with less than 100 words\n",
    "df = df[df['Partitioned Abstract'].str.split().apply(len) >= 100].reset_index(drop=True)\n",
    "\n",
    "\n",
    "x_train_bow = count_vect.fit_transform(df['Partitioned Abstract'])\n",
    "df\n",
    "#x_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7009481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de454dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743, 7810)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF transformation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(x_train_bow)\n",
    "X_train_tf = tf_transformer.transform(x_train_bow)\n",
    "X_train_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4a0a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743, 7810)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(x_train_bow)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e6eeb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/etienne/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Doc2vec transformation\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "tagged_data = [\n",
    "    TaggedDocument(\n",
    "        words=word_tokenize(row[\"Partitioned Abstract\"].lower()), tags=[str(i)]\n",
    "    )\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "model = Doc2Vec(vector_size=100, min_count=2, epochs=100, workers=4, window=5)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Get the document vectors\n",
    "X_train_doc_vectors = [model.dv[str(i)] for i in range(len(tagged_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1c0b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667ee22e70cd4b129427c01d21a38f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 1024])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "task = \"classification\"\n",
    "\n",
    "embeddings = model.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1ac42b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Training using SVM---\n",
      "Bag of words:\n",
      "{'train': {'accuracy': 0.5280898876404494,\n",
      "           'precision': 0.5543246537930471,\n",
      "           'recall': 0.5280898876404494,\n",
      "           'f1': 0.5070830537544314},\n",
      " 'test': {'accuracy': 0.5604026845637584,\n",
      "          'precision': 0.6172395740585587,\n",
      "          'recall': 0.5604026845637584,\n",
      "          'f1': 0.5470042661266374}}\n",
      "TFIDF: \n",
      "{'train': {'accuracy': 0.5258426966292135,\n",
      "           'precision': 0.5474942855247366,\n",
      "           'recall': 0.5258426966292135,\n",
      "           'f1': 0.5091650500591278},\n",
      " 'test': {'accuracy': 0.5671140939597316,\n",
      "          'precision': 0.6000971220721266,\n",
      "          'recall': 0.5671140939597316,\n",
      "          'f1': 0.5543097423253789}}\n",
      "Doc2Vec:\n",
      "{'train': {'accuracy': 0.7191011235955056,\n",
      "           'precision': 0.7168506642952999,\n",
      "           'recall': 0.7191011235955056,\n",
      "           'f1': 0.7169635524146218},\n",
      " 'test': {'accuracy': 0.7214765100671141,\n",
      "          'precision': 0.7188820490926718,\n",
      "          'recall': 0.7214765100671141,\n",
      "          'f1': 0.7190100449811745}}\n",
      "---Training using NaiveBayes---\n",
      "Bag of words:\n",
      "{'train': {'accuracy': 0.7438202247191011,\n",
      "           'precision': 0.739391695313737,\n",
      "           'recall': 0.7438202247191011,\n",
      "           'f1': 0.7291450512595774},\n",
      " 'test': {'accuracy': 0.7248322147651006,\n",
      "          'precision': 0.7186102984760703,\n",
      "          'recall': 0.7248322147651006,\n",
      "          'f1': 0.7103722736927075}}\n",
      "TFIDF: \n",
      "{'train': {'accuracy': 0.6651685393258427,\n",
      "           'precision': 0.6415255658194952,\n",
      "           'recall': 0.6651685393258427,\n",
      "           'f1': 0.6135837746006166},\n",
      " 'test': {'accuracy': 0.6879194630872483,\n",
      "          'precision': 0.7556018926606345,\n",
      "          'recall': 0.6879194630872483,\n",
      "          'f1': 0.6554766366065599}}\n",
      "Doc2Vec:\n",
      "{'train': {'accuracy': 0.7033707865168539,\n",
      "           'precision': 0.707926523947446,\n",
      "           'recall': 0.7033707865168539,\n",
      "           'f1': 0.7045906213113816},\n",
      " 'test': {'accuracy': 0.7013422818791947,\n",
      "          'precision': 0.7121406493991055,\n",
      "          'recall': 0.7013422818791947,\n",
      "          'f1': 0.7036506783395049}}\n",
      "--Training using embeddings and logistic regression---\n",
      "{'train': {'accuracy': 0.82996632996633,\n",
      "           'precision': 0.8331374663103267,\n",
      "           'recall': 0.82996632996633,\n",
      "           'f1': 0.8299884980469119},\n",
      " 'test': {'accuracy': 0.7516778523489933,\n",
      "          'precision': 0.7802593732653781,\n",
      "          'recall': 0.7516778523489933,\n",
      "          'f1': 0.7524933120570704}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "def trainSVM(partition, label, folds):\n",
    "    partition_train, partition_test, label_train, label_test = train_test_split(\n",
    "        partition, label, test_size=0.4, random_state=42, stratify=label\n",
    "    )\n",
    "\n",
    "    clf = make_pipeline(preprocessing.StandardScaler(with_mean=False), svm.SVC(kernel='sigmoid', C=1))\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Cross-validation on training data\n",
    "    label_pred_cv = cross_val_predict(clf, partition_train, label_train, cv=cv)\n",
    "\n",
    "    # Fit final model and predict on test set\n",
    "    clf.fit(partition_train, label_train)\n",
    "    label_pred_test = clf.predict(partition_test)\n",
    "\n",
    "    # Metrics\n",
    "    results = {\n",
    "        \"train\": {\n",
    "            \"accuracy\": accuracy_score(label_train, label_pred_cv),\n",
    "            \"precision\": precision_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(label_test, label_pred_test),\n",
    "            \"precision\": precision_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def trainNB(partition, label, folds):\n",
    "    partition_train, partition_test, label_train, label_test = train_test_split(\n",
    "        partition, label, test_size=0.4, random_state=42, stratify=label\n",
    "    )\n",
    "\n",
    "    clf = make_pipeline(MultinomialNB())\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Cross-validation on training data\n",
    "    label_pred_cv = cross_val_predict(clf, partition_train, label_train, cv=cv)\n",
    "\n",
    "    # Fit final model and predict on test set\n",
    "    clf.fit(partition_train, label_train)\n",
    "    label_pred_test = clf.predict(partition_test)\n",
    "\n",
    "    # Metrics\n",
    "    results = {\n",
    "        \"train\": {\n",
    "            \"accuracy\": accuracy_score(label_train, label_pred_cv),\n",
    "            \"precision\": precision_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(label_test, label_pred_test),\n",
    "            \"precision\": precision_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "def trainGNB(partition, label, folds):\n",
    "    partition_train, partition_test, label_train, label_test = train_test_split(\n",
    "        partition, label, test_size=0.4, random_state=42, stratify=label\n",
    "    )\n",
    "\n",
    "    clf = make_pipeline(GaussianNB())\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Cross-validation on training data\n",
    "    label_pred_cv = cross_val_predict(clf, partition_train, label_train, cv=cv)\n",
    "\n",
    "    # Fit final model and predict on test set\n",
    "    clf.fit(partition_train, label_train)\n",
    "    label_pred_test = clf.predict(partition_test)\n",
    "\n",
    "    # Metrics\n",
    "    results = {\n",
    "        \"train\": {\n",
    "            \"accuracy\": accuracy_score(label_train, label_pred_cv),\n",
    "            \"precision\": precision_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_train, label_pred_cv, average='weighted', zero_division=0),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(label_test, label_pred_test),\n",
    "            \"precision\": precision_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label_test, label_pred_test, average='weighted', zero_division=0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def train_embedding_classifier(X, y, folds=10):\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    clf = LogisticRegression(solver='liblinear')  # You can try other solvers too\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Cross-validation predictions on training data\n",
    "    y_pred_cv = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Final fit + test set prediction\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"train\": {\n",
    "            \"accuracy\": accuracy_score(y_train, y_pred_cv),\n",
    "            \"precision\": precision_score(y_train, y_pred_cv, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(y_train, y_pred_cv, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(y_train, y_pred_cv, average='weighted', zero_division=0),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "            \"precision\": precision_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "#Call the function\n",
    "print(\"---Training using SVM---\")\n",
    "print(\"Bag of words:\")\n",
    "pprint.pp(trainSVM(x_train_bow, df[\"target\"], 10))\n",
    "print(\"TFIDF: \")\n",
    "pprint.pp(trainSVM(X_train_tfidf, df[\"target\"], 10))\n",
    "print(\"Doc2Vec:\")\n",
    "pprint.pp(trainSVM(X_train_doc_vectors, df[\"target\"], 10))\n",
    "print(\"---Training using NaiveBayes---\")\n",
    "print(\"Bag of words:\")\n",
    "pprint.pp(trainNB(x_train_bow, df[\"target\"], 10))\n",
    "print(\"TFIDF: \")\n",
    "pprint.pp(trainNB(X_train_tfidf, df[\"target\"], 10))\n",
    "print(\"Doc2Vec:\")\n",
    "pprint.pp(trainGNB(X_train_doc_vectors, df[\"target\"], 10))\n",
    "print(\"--Training using embeddings and logistic regression---\")\n",
    "pprint.pp(train_embedding_classifier(embeddings.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "188b05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743, 7810)\n",
      "(743,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use Bag of Words features for train/test split\n",
    "X = x_train_bow\n",
    "y = df[\"target\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(x_train_bow.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "598ade75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6933333333333334"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\", C=1).fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0a88c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\", C=1)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=10)\n",
    "scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ac87f0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8       , 0.78666667, 0.69333333, 0.77333333, 0.78666667,\n",
       "       0.82666667, 0.78666667, 0.8       , 0.8       , 0.78666667])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "cross_val_score(clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d813e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "scaler = preprocessing.StandardScaler(with_mean=False).fit(X_train)\n",
    "x_train_transformed = scaler.transform(X_train)\n",
    "\n",
    "clf = svm.SVC(C=1).fit(x_train_transformed, y_train)\n",
    "x_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "clf.score(x_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9ef18d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3917\n",
      "Precision: 0.5179\n",
      "Recall:    0.3917\n",
      "F1 Score:  0.3828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "\n",
    "clf = make_pipeline(preprocessing.StandardScaler(with_mean=False), svm.SVC(C=1))\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "cross_val_score(clf, X, y, cv=cv)\n",
    "\n",
    "scores = []\n",
    "scores = cross_val_score\n",
    "\n",
    "# Perform cross-validation and get predictions for each fold\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
