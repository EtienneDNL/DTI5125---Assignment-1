{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46619dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Label</th>\n",
       "      <th>Partitioned Abstract</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enhancing Investment Analysis: Optimizing AI-A...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Han, XW; Wang, N; Che, SK; Yang, HY; Zhang, KP...</td>\n",
       "      <td>AI and Finance</td>\n",
       "      <td>recent years application generative artificial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial intelligence applications in supply...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Pournader, M; Ghaderi, H; Hassanzadegan, A; Fa...</td>\n",
       "      <td>AI and Supply Chain Management</td>\n",
       "      <td>paper presents systematic review studies relat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forecasting disruptions in global food value c...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tamasiga, P; Ouassou, E; Onyeaka, H; Bakwena, ...</td>\n",
       "      <td>AI and Supply Chain Management</td>\n",
       "      <td>globalization interconnected supply chains led...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI-powered marketing: What, where, and how?*</td>\n",
       "      <td>2024</td>\n",
       "      <td>Kumar, V; Ashraf, AR; Nadeem, W</td>\n",
       "      <td>AI and Marketing</td>\n",
       "      <td>artificial intelligence ai become disruptive f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence based decision-making ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Lehner, OM; Ittonen, K; Silvola, H; Strm, E; ...</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>trustworthiness using rest's component model a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>The Big Data, Artificial Intelligence, and Blo...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Gusc, J; Bosma, P; Jarka, S; Biernat-Jarka, A</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>current energy prices include environmental so...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>The Artificial Intelligence Revolution in Digi...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Al-Baity, HH</td>\n",
       "      <td>AI and Finance</td>\n",
       "      <td>artificial intelligence ai proliferated last y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>ROLE OF GENETIC-VARIATION AT THE APO AI-CIII-A...</td>\n",
       "      <td>1993</td>\n",
       "      <td>XU, CF; ANGELICO, F; DELBEN, M; HUMPHRIES, S</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>cholesterol diet however significant associati...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Exploring volatility interconnections between ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yousaf, I; Ijaz, MS; Umar, M; Li, YS</td>\n",
       "      <td>AI and Economics</td>\n",
       "      <td>energy artificial intelligence ai two top fiel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>Boosting the efficacy of green accounting for ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Khan, S; Gupta, S</td>\n",
       "      <td>AI and Accounting</td>\n",
       "      <td>accounting practices positively impact current...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Year  \\\n",
       "0    Enhancing Investment Analysis: Optimizing AI-A...  2024   \n",
       "1    Artificial intelligence applications in supply...  2021   \n",
       "2    Forecasting disruptions in global food value c...  2023   \n",
       "3         AI-powered marketing: What, where, and how?*  2024   \n",
       "4    Artificial intelligence based decision-making ...  2022   \n",
       "..                                                 ...   ...   \n",
       "738  The Big Data, Artificial Intelligence, and Blo...  2022   \n",
       "739  The Artificial Intelligence Revolution in Digi...  2023   \n",
       "740  ROLE OF GENETIC-VARIATION AT THE APO AI-CIII-A...  1993   \n",
       "741  Exploring volatility interconnections between ...  2024   \n",
       "742  Boosting the efficacy of green accounting for ...  2025   \n",
       "\n",
       "                                               Authors  \\\n",
       "0    Han, XW; Wang, N; Che, SK; Yang, HY; Zhang, KP...   \n",
       "1    Pournader, M; Ghaderi, H; Hassanzadegan, A; Fa...   \n",
       "2    Tamasiga, P; Ouassou, E; Onyeaka, H; Bakwena, ...   \n",
       "3                      Kumar, V; Ashraf, AR; Nadeem, W   \n",
       "4    Lehner, OM; Ittonen, K; Silvola, H; Strm, E; ...   \n",
       "..                                                 ...   \n",
       "738      Gusc, J; Bosma, P; Jarka, S; Biernat-Jarka, A   \n",
       "739                                       Al-Baity, HH   \n",
       "740       XU, CF; ANGELICO, F; DELBEN, M; HUMPHRIES, S   \n",
       "741               Yousaf, I; Ijaz, MS; Umar, M; Li, YS   \n",
       "742                                  Khan, S; Gupta, S   \n",
       "\n",
       "                              Label  \\\n",
       "0                    AI and Finance   \n",
       "1    AI and Supply Chain Management   \n",
       "2    AI and Supply Chain Management   \n",
       "3                  AI and Marketing   \n",
       "4                 AI and Accounting   \n",
       "..                              ...   \n",
       "738               AI and Accounting   \n",
       "739                  AI and Finance   \n",
       "740               AI and Accounting   \n",
       "741                AI and Economics   \n",
       "742               AI and Accounting   \n",
       "\n",
       "                                  Partitioned Abstract  target  \n",
       "0    recent years application generative artificial...       1  \n",
       "1    paper presents systematic review studies relat...       0  \n",
       "2    globalization interconnected supply chains led...       0  \n",
       "3    artificial intelligence ai become disruptive f...       2  \n",
       "4    trustworthiness using rest's component model a...       4  \n",
       "..                                                 ...     ...  \n",
       "738  current energy prices include environmental so...       4  \n",
       "739  artificial intelligence ai proliferated last y...       1  \n",
       "740  cholesterol diet however significant associati...       4  \n",
       "741  energy artificial intelligence ai two top fiel...       3  \n",
       "742  accounting practices positively impact current...       4  \n",
       "\n",
       "[743 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words transformation of the partitioned abstracts\n",
    "# This script uses the CountVectorizer from sklearn to transform the partitioned abstracts into a bag of words representation.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Gets the data from Data/partitioned_abstracts.csv and puts it into a pandas dataframe\n",
    "df = pd.read_csv('Data/Partitioned_Abstracts.csv', sep=',', header=0, encoding='utf-8')\n",
    "\n",
    "# Add target column to the dataframe based on label\n",
    "# This is used to reduce processing times\n",
    "label_mapping = {\n",
    "    'AI and Supply Chain Management': 0,\n",
    "    'AI and Finance': 1,\n",
    "    'AI and Marketing': 2,\n",
    "    'AI and Economics': 3,\n",
    "    'AI and Accounting': 4\n",
    "}\n",
    "df['target'] = df['Label'].map(label_mapping)\n",
    "\n",
    "#randomize the order of the dataframe using random.shuffle\n",
    "random.seed(42)  # For reproducibility\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#remove partitions with less than 100 words\n",
    "#This step can be added or removed to test accuracy with samples that are shorter than others.\n",
    "df = df[df['Partitioned Abstract'].str.split().apply(len) >= 100].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Transforming to Bag of Words.\n",
    "x_train_bow = count_vect.fit_transform(df['Partitioned Abstract'])\n",
    "df\n",
    "#x_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de454dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743, 7810)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF transformation (NO IDF)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(x_train_bow)\n",
    "X_train_tf = tf_transformer.transform(x_train_bow)\n",
    "X_train_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a0a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743, 7810)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(x_train_bow)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6eeb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shepo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Doc2vec transformation\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "tagged_data = [\n",
    "    TaggedDocument(\n",
    "        words=word_tokenize(row[\"Partitioned Abstract\"].lower()), tags=[str(i)]\n",
    "    )\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "model = Doc2Vec(vector_size=100, min_count=2, epochs=100, workers=4, window=5)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Get the document vectors\n",
    "X_train_doc_vectors = [model.dv[str(i)] for i in range(len(tagged_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c0b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 24/24 [00:31<00:00,  1.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Embedding transformation using jina embeddings v3\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "task = \"classification\"\n",
    "\n",
    "embeddings = model.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01a2a2ab-436c-4cc7-a0f9-5867f119bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 19.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 384])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "embeddings_2 = model2.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50e37470-4a31-4074-bba4-4f6aca4fa509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 24/24 [00:07<00:00,  3.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True)\n",
    "embeddings_3 = model3.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6fe844b-11af-4ad3-be0f-00188f9b44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shepo\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L3-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 32.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 384])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L3-v2\", trust_remote_code=True)\n",
    "embeddings_4 = model4.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b5ce1de-2962-4a69-b213-2ebb6c656a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/specter2_base. Creating a new one with mean pooling.\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shepo\\.cache\\huggingface\\hub\\models--allenai--specter2_base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 24/24 [00:06<00:00,  3.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([743, 768])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = SentenceTransformer(\"allenai/specter2_base\", trust_remote_code=True)\n",
    "embeddings_5 = model5.encode(\n",
    "    df[\"Partitioned Abstract\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1ac42b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Testing using SVM---\n",
      "Bag of words:\n",
      "{'Results': {'accuracy': 0.6056527590847914,\n",
      "             'precision': 0.6086747022927794,\n",
      "             'recall': 0.6056527590847914,\n",
      "             'f1': 0.6066801808574992,\n",
      "             'roc_curve_file': 'roc_curves/svm_BOW_20250528_050310.jpg'}}\n",
      "TFIDF: \n",
      "{'Results': {'accuracy': 0.6056527590847914,\n",
      "             'precision': 0.6097571247856168,\n",
      "             'recall': 0.6056527590847914,\n",
      "             'f1': 0.6070027166979564,\n",
      "             'roc_curve_file': 'roc_curves/svm_TFIDF_20250528_050335.jpg'}}\n",
      "Doc2Vec:\n",
      "{'Results': {'accuracy': 0.7362045760430687,\n",
      "             'precision': 0.7363639463824524,\n",
      "             'recall': 0.7362045760430687,\n",
      "             'f1': 0.736054417961173,\n",
      "             'roc_curve_file': 'roc_curves/svm_Doc2Vec_20250528_050336.jpg'}}\n",
      "---Testing using NaiveBayes---\n",
      "Bag of words:\n",
      "{'Results': {'accuracy': 0.7442799461641992,\n",
      "             'precision': 0.737457537898254,\n",
      "             'recall': 0.7442799461641992,\n",
      "             'f1': 0.7343887328615553,\n",
      "             'roc_curve_file': 'roc_curves/nb_BOW_20250528_050336.jpg'}}\n",
      "TFIDF: \n",
      "{'Results': {'accuracy': 0.7079407806191117,\n",
      "             'precision': 0.7189982391036734,\n",
      "             'recall': 0.7079407806191117,\n",
      "             'f1': 0.6675077153846348,\n",
      "             'roc_curve_file': 'roc_curves/nb_TFIDF_20250528_050336.jpg'}}\n",
      "Doc2Vec:\n",
      "{'Results': {'accuracy': 0.7227456258411844,\n",
      "             'precision': 0.7315774847518248,\n",
      "             'recall': 0.7227456258411844,\n",
      "             'f1': 0.724529632780135,\n",
      "             'roc_curve_file': 'roc_curves/GNB_Doc2Vec_20250528_050336.jpg'}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pprint\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Train an SVM classifier and generate evaluation metrics and ROC curves\n",
    "def trainSVM(partition, label, folds, model):\n",
    "    # Build a pipeline with standard scaling and SVM with sigmoid kernel\n",
    "    clf = make_pipeline(preprocessing.StandardScaler(with_mean=False), svm.SVC(kernel='sigmoid', C=1, probability=True))\n",
    "    \n",
    "    # Create k-fold cross-validator\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validated prediction to get class probabilities\n",
    "    label_pred_proba = cross_val_predict(clf, partition, label, cv=cv, method='predict_proba')\n",
    "    \n",
    "    # Convert probabilities to predicted class labels\n",
    "    label_pred_test = np.argmax(label_pred_proba, axis=1)\n",
    "\n",
    "    # Generate ROC curve for each class\n",
    "    classes = np.unique(label)\n",
    "    label_bin = label_binarize(label, classes=classes)\n",
    "    plt.figure()\n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(label_bin[:, i], label_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'Class {class_label} (area = {roc_auc:.2f})')\n",
    "\n",
    "    # Plot diagonal line for random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'OvR ROC Curve - SVM & {model}')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Save the ROC curve image\n",
    "    if not os.path.exists('roc_curves'):\n",
    "        os.makedirs('roc_curves')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'roc_curves/svm_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Return evaluation metrics\n",
    "    results = {\n",
    "        \"Results\": {\n",
    "            \"accuracy\": accuracy_score(label, label_pred_test),\n",
    "            \"precision\": precision_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"roc_curve_file\": filename\n",
    "        }\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier and evaluate it\n",
    "def trainNB(partition, label, folds, model):\n",
    "    # Create pipeline with MultinomialNB (no scaling needed)\n",
    "    clf = make_pipeline(MultinomialNB())\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    label_pred_proba = cross_val_predict(clf, partition, label, cv=cv, method='predict_proba')\n",
    "    label_pred_test = np.argmax(label_pred_proba, axis=1)\n",
    "\n",
    "    classes = np.unique(label)\n",
    "    label_bin = label_binarize(label, classes=classes)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(label_bin[:, i], label_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'Class {class_label} (area = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'OvR ROC Curve - MultinomialNB & {model}')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    if not os.path.exists('roc_curves'):\n",
    "        os.makedirs('roc_curves')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'roc_curves/nb_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Return evaluation metrics\n",
    "    results = {\n",
    "        \"Results\": {\n",
    "            \"accuracy\": accuracy_score(label, label_pred_test),\n",
    "            \"precision\": precision_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"roc_curve_file\": filename\n",
    "        }\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Train a Gaussian Naive Bayes classifier and evaluate it\n",
    "def trainGNB(partition, label, folds, model):\n",
    "    clf = make_pipeline(GaussianNB())\n",
    "    cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    label_pred_proba = cross_val_predict(clf, partition, label, cv=cv, method='predict_proba')\n",
    "    label_pred_test = np.argmax(label_pred_proba, axis=1)\n",
    "\n",
    "    classes = np.unique(label)\n",
    "    label_bin = label_binarize(label, classes=classes)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    for i, class_label in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(label_bin[:, i], label_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'Class {class_label} (area = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'OvR ROC Curve - GaussianNB & {model}')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    if not os.path.exists('roc_curves'):\n",
    "        os.makedirs('roc_curves')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'roc_curves/GNB_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Return evaluation metrics\n",
    "    results = {\n",
    "        \"Results\": {\n",
    "            \"accuracy\": accuracy_score(label, label_pred_test),\n",
    "            \"precision\": precision_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(label, label_pred_test, average='weighted', zero_division=0),\n",
    "            \"roc_curve_file\": filename\n",
    "        }\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the function\n",
    "print(\"---Testing using SVM---\")\n",
    "print(\"Bag of words:\")\n",
    "pprint.pp(trainSVM(x_train_bow, df[\"target\"], 10, \"BOW\"))\n",
    "print(\"TFIDF: \")\n",
    "pprint.pp(trainSVM(X_train_tfidf, df[\"target\"], 10, \"TFIDF\"))\n",
    "print(\"Doc2Vec:\")\n",
    "pprint.pp(trainSVM(X_train_doc_vectors, df[\"target\"], 10, \"Doc2Vec\"))\n",
    "print(\"---Testing using NaiveBayes---\")\n",
    "print(\"Bag of words:\")\n",
    "pprint.pp(trainNB(x_train_bow, df[\"target\"], 10, \"BOW\"))\n",
    "print(\"TFIDF: \")\n",
    "pprint.pp(trainNB(X_train_tfidf, df[\"target\"], 10, \"TFIDF\"))\n",
    "print(\"Doc2Vec:\")\n",
    "pprint.pp(trainGNB(X_train_doc_vectors, df[\"target\"], 10, \"Doc2Vec\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a29625e3-8103-4001-8fd8-1657cd1d2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Testing using embeddings (JinAI) and logistic regression---\n",
      "{'Results': {'accuracy': 0.8250336473755047,\n",
      "             'precision': 0.8316393717984946,\n",
      "             'recall': 0.8250336473755047,\n",
      "             'f1': 0.8258820743562051,\n",
      "             'roc_curve_file': 'roc_curves/embeddings_EmbeddingsLR_JinAI_20250528_050634.jpg'}}\n",
      "--Testing using embeddings (all-miniLm-L6-v2) and logistic regression---\n",
      "{'Results': {'accuracy': 0.8048452220726783,\n",
      "             'precision': 0.8081797632323277,\n",
      "             'recall': 0.8048452220726783,\n",
      "             'f1': 0.8051420716274219,\n",
      "             'roc_curve_file': 'roc_curves/embeddings_EmbeddingsLR_all-miniLm-L6-v2_20250528_050634.jpg'}}\n",
      "--Testing using embeddings (all-mpnet-base) and logistic regression---\n",
      "{'Results': {'accuracy': 0.8088829071332436,\n",
      "             'precision': 0.8139378552038442,\n",
      "             'recall': 0.8088829071332436,\n",
      "             'f1': 0.8096349418093295,\n",
      "             'roc_curve_file': 'roc_curves/embeddings_EmbeddingsLR_all-mpnet-base_20250528_050635.jpg'}}\n",
      "--Testing using embeddings (paraphrase-MiniLM-L3-v2) and logistic regression---\n",
      "{'Results': {'accuracy': 0.7362045760430687,\n",
      "             'precision': 0.7378971608398345,\n",
      "             'recall': 0.7362045760430687,\n",
      "             'f1': 0.7369069962615068,\n",
      "             'roc_curve_file': 'roc_curves/embeddings_EmbeddingsLR_paraphrase-MiniLM-L3-v2_20250528_050636.jpg'}}\n",
      "--Testing using embeddings (allenai/specter2) and logistic regression---\n",
      "{'Results': {'accuracy': 0.7738896366083445,\n",
      "             'precision': 0.7767393266676363,\n",
      "             'recall': 0.7738896366083445,\n",
      "             'f1': 0.7748382779323348,\n",
      "             'roc_curve_file': 'roc_curves/embeddings_EmbeddingsLR_specter2_20250528_050638.jpg'}}\n"
     ]
    }
   ],
   "source": [
    "# Assuming these are your result dictionaries from previous runs:\n",
    "res1 = train_embedding_classifier(embeddings.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10, \"EmbeddingsLR_JinAI\")\n",
    "res2 = train_embedding_classifier(embeddings_2.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10, \"EmbeddingsLR_all-miniLm-L6-v2\")\n",
    "res3 = train_embedding_classifier(embeddings_3.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10, \"EmbeddingsLR_all-mpnet-base\")\n",
    "res4 = train_embedding_classifier(embeddings_4.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10, \"EmbeddingsLR_paraphrase-MiniLM-L3-v2\")\n",
    "res5 = train_embedding_classifier(embeddings_5.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], 10, \"EmbeddingsLR_specter2\")\n",
    "\n",
    "print(\"--Testing using embeddings (JinAI) and logistic regression---\")\n",
    "pprint.pp(res1)\n",
    "print(\"--Testing using embeddings (all-miniLm-L6-v2) and logistic regression---\")\n",
    "pprint.pp(res2)\n",
    "print(\"--Testing using embeddings (all-mpnet-base) and logistic regression---\")\n",
    "pprint.pp(res3)\n",
    "print(\"--Testing using embeddings (paraphrase-MiniLM-L3-v2) and logistic regression---\")\n",
    "pprint.pp(res4)\n",
    "print(\"--Testing using embeddings (allenai/specter2) and logistic regression---\")\n",
    "pprint.pp(res5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Collect accuracy and F1 results\n",
    "results_accuracy = [\n",
    "    (\"JinAI\", res1[\"Results\"][\"accuracy\"]),\n",
    "    (\"all-miniLm-L6-v2\", res2[\"Results\"][\"accuracy\"]),\n",
    "    (\"all-mpnet-base\", res3[\"Results\"][\"accuracy\"]),\n",
    "    (\"paraphrase-MiniLM-L3-v2\", res4[\"Results\"][\"accuracy\"]),\n",
    "    (\"specter2\", res5[\"Results\"][\"accuracy\"])\n",
    "]\n",
    "\n",
    "results_f1 = [\n",
    "    (\"JinAI\", res1[\"Results\"][\"f1\"]),\n",
    "    (\"all-miniLm-L6-v2\", res2[\"Results\"][\"f1\"]),\n",
    "    (\"all-mpnet-base\", res3[\"Results\"][\"f1\"]),\n",
    "    (\"paraphrase-MiniLM-L3-v2\", res4[\"Results\"][\"f1\"]),\n",
    "    (\"specter2\", res5[\"Results\"][\"f1\"])\n",
    "]\n",
    "\n",
    "# --- Plot Accuracy with Labels ---\n",
    "labels, accuracies = zip(*results_accuracy)\n",
    "\n",
    "plt.figure(figsize=(10,6), dpi=150)  # Higher DPI for sharper image\n",
    "bars = plt.bar(labels, accuracies, color='teal')\n",
    "\n",
    "# Add value labels (rounded to 3 decimal places)\n",
    "plt.bar_label(bars, labels=[f\"{acc:.3f}\" for acc in accuracies], padding=5, fontsize=10)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Logistic Regression Across Different Embedding Models')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "if not os.path.exists('Embedding Models'):\n",
    "    os.makedirs('Embedding Models')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'Embedding Models/Accuracy_Embedding_Models_{timestamp}.jpg'\n",
    "plt.savefig(filename, dpi=300)  # Higher DPI for sharper image\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "def0d6a6-4082-4d2a-ad2d-aa7e5afb0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "def test_logistic_C(X, y, C_values,  model, folds=10):\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for C_val in C_values:\n",
    "        clf = LogisticRegression(C=C_val, solver='lbfgs', max_iter=1000)\n",
    "        cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "        label_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "        accuracy = accuracy_score(y, label_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        print(f\"C={C_val}: Accuracy={accuracy:.4f}\")\n",
    "\n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar([str(C) for C in C_values], accuracy_scores, color='skyblue')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"C value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. C in Logistic Regression\")\n",
    "    if not os.path.exists('hyperParameter_tuning'):\n",
    "        os.makedirs('hyperParameter_tuning')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'hyperParameter_tuning/GNB_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59d83206-3fed-4ab7-997c-2b7a3d283be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_solver(X, y, solvers,  model,folds=10):\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for solver in solvers:\n",
    "        clf = LogisticRegression(solver=solver, max_iter=1000)\n",
    "        cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "        label_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "        accuracy = accuracy_score(y, label_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        print(f\"Solver={solver}: Accuracy={accuracy:.7f}\")\n",
    "\n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(solvers, accuracy_scores, color='lightgreen')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Solver\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. Solver in Logistic Regression\")\n",
    "    if not os.path.exists('hyperParameter_tuning'):\n",
    "        os.makedirs('hyperParameter_tuning')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'hyperParameter_tuning/GNB_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0487679-94cc-440f-ab5a-6d5c127c8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_penalty(X, y, penalties, model,folds=10):\n",
    "    accuracies = []\n",
    "    for penalty in penalties:\n",
    "        clf = LogisticRegression(solver='saga', penalty=penalty, max_iter=1000)  # 'saga' supports all penalties\n",
    "        cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "        y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        accuracies.append(acc)\n",
    "        print(f\"Penalty={penalty}: Accuracy={acc:.4f}\")\n",
    "\n",
    "    plt.bar(penalties, accuracies, color='orange')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Accuracy vs Penalty')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if not os.path.exists('hyperParameter_tuning'):\n",
    "        os.makedirs('hyperParameter_tuning')\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'hyperParameter_tuning/GNB_{model}_{timestamp}.jpg'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c1b93d1a-036a-4200-967a-6b3cd8c83091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01: Accuracy=0.6285\n",
      "C=0.1: Accuracy=0.7981\n",
      "C=1: Accuracy=0.8250\n",
      "C=10: Accuracy=0.8129\n",
      "C=15: Accuracy=0.8048\n",
      "C=20: Accuracy=0.8008\n",
      "Solver=liblinear: Accuracy=0.8223419\n",
      "Solver=lbfgs: Accuracy=0.8250336\n",
      "Solver=sag: Accuracy=0.8250336\n",
      "Solver=saga: Accuracy=0.8250336\n",
      "Solver=newton-cg: Accuracy=0.8250336\n",
      "Penalty=l1: Accuracy=0.7699\n",
      "Penalty=l2: Accuracy=0.8250\n"
     ]
    }
   ],
   "source": [
    "c_values = [0.01, 0.1, 1, 10, 15, 20]\n",
    "test_logistic_C(embeddings.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], c_values,\"EmbeddingsLR_JinAI\", 10)\n",
    "solvers = ['liblinear', 'lbfgs', 'sag', 'saga', 'newton-cg']\n",
    "test_logistic_solver(embeddings.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], solvers,\"EmbeddingsLR_JinAI\", 10)\n",
    "penalties=['l1', 'l2']\n",
    "test_logistic_penalty(embeddings.to(dtype=torch.float32).cpu().numpy(), df[\"target\"], penalties,\"EmbeddingsLR_JinAI\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b24ab1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP summary plot saved to: shap_outputs/shap_summary_20250528_050145.jpg\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "\n",
    "def explain_embedding_classifier_with_shap(embeddings, labels):\n",
    "    # Convert embeddings to numpy\n",
    "    X = embeddings.to(dtype=torch.float32).cpu().numpy()\n",
    "    y = labels\n",
    "\n",
    "    # Train Logistic Regression on all data\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Create SHAP explainer for linear model\n",
    "    explainer = shap.Explainer(model, X, feature_perturbation=\"interventional\")\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    # Generate feature names: dim_0, dim_1, ..., dim_n\n",
    "    feature_names = [f\"dim_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "    # Plot and save SHAP summary plot\n",
    "    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)\n",
    "\n",
    "    # Save the plot\n",
    "    if not os.path.exists(\"shap_outputs\"):\n",
    "        os.makedirs(\"shap_outputs\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"shap_outputs/shap_summary_{timestamp}.jpg\"\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leaves space at top for title\n",
    "    plt.title(f'SHAP - Champion Model')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"SHAP summary plot saved to: {filename}\")\n",
    "    \n",
    "\n",
    "    return shap_values\n",
    "\n",
    "shap_values = explain_embedding_classifier_with_shap(embeddings, df[\"target\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
